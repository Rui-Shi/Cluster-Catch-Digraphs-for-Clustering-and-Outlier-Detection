data.num = lapply(1:iteN, function(x){
return(data.listNum[[x]]$num)
})
cl <- makeCluster(cores)
registerDoParallel(cl)
labels_list = foreach(x=data.list,.packages = c("MASS","cluster","igraph")) %dopar% MST_Outlier(x, cont, R)
stopCluster(cl)
count.result = foreach(x=1:iteN,.combine = rbind) %do% count_MST2(data.num[[x]][1], data.num[[x]][2], labels_list[[x]])
mean = c(mean(count.result[,1]),mean(count.result[,2]),mean(count.result[,3]),mean(count.result[,4]))
print(paste("MST: the mean TPR is", mean[1],",","the mean TNR", mean[2],",","the mean BA", mean[3],",","the mean F2", mean[4]))
tt2 = Sys.time()
tt2 - tt1
tt1 = Sys.time()
source("G:/code_working_folder/general functions/Uni-Gau_cls.R")
source("G:/code_working_folder/general functions/ratio1.R")
source("G:/code_working_folder/Algo_Compare/MST/MST_Outlier.R")
library(parallel)
library(doParallel)
library(MASS)
library(igraph)
library(dbscan)
d = 10
iteN = 1000
cores = detectCores()
# cores = 24 # 13900K
# simulation settings
kappa1 = 6
mu1 = ratio[4]
expand1 = 0
r = 0.1
kappa2 = 0
scale = 0.005
mu2 = ratio[4]
expand2 = 0
slen = 1
kappa_O = 20
R = 1.3 # thresh: the ratio to cut a edge when comparing its adjacent edges
cont = 0.03 # cont: the minimal % of a cluster
# simulate clusters of random sizes and positions
set.seed(1234)
data.listNum = lapply(1:iteN, function(x){
data_simu =  Uni.Gau_cls(d, kappa1, r, mu1, expand1, kappa2, scale, mu2, expand2, slen, kappa_O)
cls1 = data_simu$Matérn_children
cls2 = data_simu$Thomas_children
cls3 = data_simu$noise
outlier = data_simu$Outlier
return(c(data = list(rbind(cls1, cls2, cls3, outlier)), num = list(data_simu$num)))
})
data.list = lapply(1:iteN, function(x){
return(data.listNum[[x]]$data)
})
data.num = lapply(1:iteN, function(x){
return(data.listNum[[x]]$num)
})
cl <- makeCluster(cores)
registerDoParallel(cl)
labels_list = foreach(x=data.list,.packages = c("MASS","cluster","igraph")) %dopar% MST_Outlier(x, cont, R)
stopCluster(cl)
count.result = foreach(x=1:iteN,.combine = rbind) %do% count_MST2(data.num[[x]][1], data.num[[x]][2], labels_list[[x]])
mean = c(mean(count.result[,1]),mean(count.result[,2]),mean(count.result[,3]),mean(count.result[,4]))
print(paste("MST: the mean TPR is", mean[1],",","the mean TNR", mean[2],",","the mean BA", mean[3],",","the mean F2", mean[4]))
tt2 = Sys.time()
tt2 - tt1
tt1 = Sys.time()
source("G:/code_working_folder/general functions/Uni-Gau_cls.R")
source("G:/code_working_folder/general functions/ratio1.R")
source("G:/code_working_folder/Algo_Compare/MST/MST_Outlier.R")
library(parallel)
library(doParallel)
library(MASS)
library(igraph)
library(dbscan)
d = 10
iteN = 1000
cores = detectCores()
# cores = 24 # 13900K
# simulation settings
kappa1 = 6
mu1 = ratio[4]
expand1 = 0
r = 0.1
kappa2 = 0
scale = 0.005
mu2 = ratio[4]
expand2 = 0
slen = 1
kappa_O = 20
R = 1.3 # thresh: the ratio to cut a edge when comparing its adjacent edges
cont = 0.03 # cont: the minimal % of a cluster
# simulate clusters of random sizes and positions
set.seed(1234)
data.listNum = lapply(1:iteN, function(x){
data_simu =  Uni.Gau_cls(d, kappa1, r, mu1, expand1, kappa2, scale, mu2, expand2, slen, kappa_O)
cls1 = data_simu$Matérn_children
cls2 = data_simu$Thomas_children
cls3 = data_simu$noise
outlier = data_simu$Outlier
return(c(data = list(rbind(cls1, cls2, cls3, outlier)), num = list(data_simu$num)))
})
data.list = lapply(1:iteN, function(x){
return(data.listNum[[x]]$data)
})
data.num = lapply(1:iteN, function(x){
return(data.listNum[[x]]$num)
})
cl <- makeCluster(cores)
registerDoParallel(cl)
labels_list = foreach(x=data.list,.packages = c("MASS","cluster","igraph")) %dopar% MST_Outlier(x, cont, R)
stopCluster(cl)
count.result = foreach(x=1:iteN,.combine = rbind) %do% count_MST2(data.num[[x]][1], data.num[[x]][2], labels_list[[x]])
mean = c(mean(count.result[,1]),mean(count.result[,2]),mean(count.result[,3]),mean(count.result[,4]))
print(paste("MST: the mean TPR is", mean[1],",","the mean TNR", mean[2],",","the mean BA", mean[3],",","the mean F2", mean[4]))
tt2 = Sys.time()
tt2 - tt1
tt1 = Sys.time()
source("G:/code_working_folder/general functions/Uni-Gau_cls.R")
source("G:/code_working_folder/general functions/ratio1.R")
source("G:/code_working_folder/Algo_Compare/MST/MST_Outlier.R")
library(parallel)
library(doParallel)
library(MASS)
library(igraph)
library(dbscan)
d = 10
iteN = 1000
cores = detectCores()
# cores = 24 # 13900K
# simulation settings
kappa1 = 6
mu1 = ratio[4]
expand1 = 0
r = 0.1
kappa2 = 0
scale = 0.005
mu2 = ratio[4]
expand2 = 0
slen = 1
kappa_O = 20
R = 1.3 # thresh: the ratio to cut a edge when comparing its adjacent edges
cont = 0.03 # cont: the minimal % of a cluster
# simulate clusters of random sizes and positions
set.seed(1234)
data.listNum = lapply(1:iteN, function(x){
data_simu =  Uni.Gau_cls(d, kappa1, r, mu1, expand1, kappa2, scale, mu2, expand2, slen, kappa_O)
cls1 = data_simu$Matérn_children
cls2 = data_simu$Thomas_children
cls3 = data_simu$noise
outlier = data_simu$Outlier
return(c(data = list(rbind(cls1, cls2, cls3, outlier)), num = list(data_simu$num)))
})
data.list = lapply(1:iteN, function(x){
return(data.listNum[[x]]$data)
})
data.num = lapply(1:iteN, function(x){
return(data.listNum[[x]]$num)
})
cl <- makeCluster(cores)
registerDoParallel(cl)
labels_list = foreach(x=data.list,.packages = c("MASS","cluster","igraph")) %dopar% MST_Outlier(x, cont, R)
stopCluster(cl)
count.result = foreach(x=1:iteN,.combine = rbind) %do% count_MST2(data.num[[x]][1], data.num[[x]][2], labels_list[[x]])
mean = c(mean(count.result[,1]),mean(count.result[,2]),mean(count.result[,3]),mean(count.result[,4]))
print(paste("MST: the mean TPR is", mean[1],",","the mean TNR", mean[2],",","the mean BA", mean[3],",","the mean F2", mean[4]))
tt2 = Sys.time()
tt2 - tt1
0.75-0.8
-0.05/sqrt(0.8*0.2/200)
pnorm(-1.768)
x <- 150  # Number of successes (satisfied customers)
n <- 200  # Sample size
p0 <- 0.8 # Hypothesized population proportion
# Conduct the test (two-sided)
prop.test(x, n, p = p0, alternative = "two.sided")
prop.test(x, n, p = p0, alternative = "two.sided", correct = FALSE)
1.7^2
binom.test(x, n, p = p0, alternative = "two.sided")
(0.75-0.8)/sqrt(0.8*0.2/200)
(0.75-0.8)/sqrt(0.8*0.2/200)
pnorm((0.75-0.8)/sqrt(0.8*0.2/200))*2
prop.test(x, n, p = p0, alternative = "two.sided", correct = FALSE)
# Old design data
x1 = 50  # Number of conversions (successes)
n1 = 1000  # Total visitors
# New design data
x2 = 84
n2 = 1200
# Test if the new design has a higher conversion rate (one-sided)
prop.test(c(x1, x2), c(n1, n2), alternative = "less", correct = F)
sqrt(3.8144)
# compute manunally
(50/1000-84/1200)/sqrt((50+84)/(1000+1200)*(1-(50+84)/(1000+1200))*(1/1000+1/1200))
z2=(50/1000-84/1200)/sqrt((50+84)/(1000+1200)*(1-(50+84)/(1000+1200))*(1/1000+1/1200)) #z-score
z2
qnorm(z2)
z2
z2=(50/1000-84/1200)/sqrt((50+84)/(1000+1200)*(1-(50+84)/(1000+1200))*(1/1000+1/1200)) #z-score
z2
pnorm(z2)
library(installr)
install.packages("installr")
library(installr)
updateR()
a=c(1,2,3)
b=a
b[3]=4
# 1 represent regular observations, 0 are outliers
library(R.matlab)
library(foreign)
library(dplyr)
setwd("G:/code_working_folder/Algo_Compare/Real Datasets")
# setwd("/media/rui/exNVME/code_working_folder/Algo_Compare/Real Datasets")
# a robust version of normalization
scale_R = function(x){
M = median(x)
madn = mad(x)
return((x-M)/madn)
}
##### glass #####
glass = readMat("glass.mat")
# normalization
X = glass$X
X = apply(glass$X, 2, scale)
# X = X[,!colSums(is.na(X)) > 0]
glass$y=ifelse(glass$y == "1", 0, 1)
glass = cbind(X, glass$y)
# remove duplicated rows
glass = as.matrix(distinct(as.data.frame(glass)))
# move outliers to the end
glass = glass[order(glass[,9], decreasing=T),]
# glass[,9]
anyNA(glass)
##### vertebral #####
vertebral = readMat("vertebral.mat")
# normalization
X = apply(vertebral$X, 2, scale_R)
vertebral$y=ifelse(vertebral$y == "1", 0, 1)
vertebral = cbind(X, vertebral$y)
# remove duplicated rows
vertebral = as.matrix(distinct(as.data.frame(vertebral)))
# move outliers to the end
vertebral = vertebral[order(vertebral[,7], decreasing=T),]
# vertebral[,7]
anyNA(vertebral)
##### vowels #####
vowels = readMat("vowels.mat")
# normalization
X = apply(vowels$X, 2, scale_R)
vowels$y=ifelse(vowels$y == "1", 0, 1)
vowels = cbind(X, vowels$y)
# remove duplicated rows
vowels = as.matrix(distinct(as.data.frame(vowels)))
# move outliers to the end
vowels = vowels[order(vowels[,13], decreasing=T),]
# vowels[,13]
anyNA(vowels)
##### thyroid #####
thyroid = readMat("thyroid.mat")
# normalization
X = apply(thyroid$X, 2, scale_R)
thyroid$y=ifelse(thyroid$y == "1", 0, 1)
thyroid = cbind(X, thyroid$y)
# remove duplicated rows
thyroid = as.matrix(distinct(as.data.frame(thyroid)))
# move outliers to the end
thyroid = thyroid[order(thyroid[, 7], decreasing=T),]
# thyroid[,13]
anyNA(thyroid)
##### ecoli #####
ecoli = read.csv("ecoli.csv")
# normalization
# X = apply(subset(ecoli, select = -class), 2, scale_R)
X = subset(ecoli, select=-class)
# X = X[,!colSums(is.na(X)) > 0]
ecoli$class=ifelse(ecoli$class == "1", 0, 1)
ecoli = cbind(X, ecoli$class)
# remove duplicated rows
ecoli = as.matrix(distinct(as.data.frame(ecoli)))
# move outliers to the end
ecoli = ecoli[order(ecoli[,6], decreasing=T),]
# ecoli[,8]
anyNA(ecoli)
##### PenDigits #####
PenDigits = read.arff('PenDigits_withoutdupl_norm_v01.arff')
# remove index col
PenDigits$id=NULL
PenDigits$outlier = ifelse(PenDigits$outlier == "yes", 0, 1)
# normalization
X = apply(subset(PenDigits, select = -outlier), 2, scale_R)
PenDigits = cbind(X, PenDigits$outlier)
# remove duplicated rows
PenDigits = as.matrix(distinct(as.data.frame(PenDigits)))
# move outliers to the end
PenDigits = PenDigits[order(PenDigits[,17], decreasing=T),]
# tail(PenDigits[,17])
anyNA(PenDigits)
index_outliers = which(PenDigits[,17]==0)
index_regular = which(PenDigits[,17]==1)
set.seed(123)
index_regular_R = sample(index_regular, 3180, replace = FALSE, prob = NULL)
PenDigits = PenDigits[c(index_regular_R, index_outliers),]
remove(index_regular)
remove(index_outliers)
remove(index_regular_R)
##### shuttle #####
shuffle = read.arff('Shuttle_withoutdupl_norm_v01.arff')
# remove index col
shuffle$id=NULL
shuffle$outlier = ifelse(shuffle$outlier == "yes", 0, 1)
# normalization
X = apply(subset(shuffle, select = -outlier), 2, scale)
# X = subset(shuffle, select = -outlier)
shuffle = cbind(X, shuffle$outlier)
# remove duplicated rows
shuffle = as.matrix(distinct(as.data.frame(shuffle)))
# move outliers to the end
shuffle = shuffle[order(shuffle[,10], decreasing=T),]
# tail(shuffle[,10])
anyNA(shuffle)
##### Waveform #####
waveform = read.arff('Waveform_withoutdupl_v01.arff')
# remove index col
waveform$id=NULL
waveform$outlier = ifelse(waveform$outlier == "yes", 0, 1)
# normalization
X = apply(subset(waveform, select = -outlier), 2, scale_R)
waveform = cbind(X, waveform$outlier)
# remove duplicated rows
waveform = as.matrix(distinct(as.data.frame(waveform)))
# move outliers to the end
waveform = waveform[order(waveform[,22], decreasing=T),]
anyNA(waveform)
##### wilt #####
wilt = read.arff('Wilt_withoutdupl_05.arff')
# remove index col
wilt$id=NULL
wilt$outlier = ifelse(wilt$outlier == "yes", 0, 1)
# normalization
X = apply(subset(wilt, select = -outlier), 2, scale_R)
wilt = cbind(X, wilt$outlier)
# remove duplicated rows
wilt = as.matrix(distinct(as.data.frame(wilt)))
# move outliers to the end
wilt = wilt[order(wilt[,6], decreasing=T),]
anyNA(wilt)
##### WBC #####
WBC = read.arff('WBC_withoutdupl_norm_v02.arff')
# remove index col
WBC$id=NULL
WBC$outlier = ifelse(WBC$outlier == "yes", 0, 1)
# normalization
# X = apply(subset(WBC, select = -outlier), 2, scale)
X = subset(WBC, select=-outlier)
WBC = cbind(X, WBC$outlier)
# remove duplicated rows
WBC = as.matrix(distinct(as.data.frame(WBC)))
# move outliers to the end
WBC = WBC[order(WBC[,10], decreasing=T),]
anyNA(WBC)
##### WDBC #####
WDBC = read.arff('WDBC_withoutdupl_norm_v02.arff')
# remove index col
WDBC$id=NULL
WDBC$outlier = ifelse(WDBC$outlier == "yes", 0, 1)
# normalization
# X = apply(subset(WDBC, select = -outlier), 2, scale)
X = subset(WDBC, select=-outlier)
WDBC = cbind(X, WDBC$outlier)
# remove duplicated rows
WDBC = as.matrix(distinct(as.data.frame(WDBC)))
# move outliers to the end
WDBC = WDBC[order(WDBC[,31], decreasing=T),]
anyNA(WDBC)
##### lymphography #####
lymphography = read.arff('Lymphography_withoutdupl_norm_idf.arff')
# remove index col
lymphography$id=NULL
lymphography$outlier = ifelse(lymphography$outlier == "yes", 0, 1)
# normalization
# X = apply(subset(lymphography, select = -outlier), 2, scale_R)
X = subset(lymphography, select=-outlier)
lymphography = cbind(X, lymphography$outlier)
# remove duplicated rows
lymphography = as.matrix(distinct(as.data.frame(lymphography)))
# move outliers to the end
lymphography = lymphography[order(lymphography[,19], decreasing=T),]
anyNA(lymphography)
##### pima #####
pima = read.arff('Pima_withoutdupl_norm_10_v01.arff')
# remove index col
pima$id=NULL
pima$outlier = ifelse(pima$outlier == "yes", 0, 1)
# normalization
# X = apply(subset(pima, select = -outlier), 2, scale_R)
X = subset(pima, select=-outlier)
pima = cbind(X, pima$outlier)
# remove duplicated rows
pima = as.matrix(distinct(as.data.frame(pima)))
# move outliers to the end
pima = pima[order(pima[,9], decreasing=T),]
anyNA(pima)
##### stamps #####
stamps = read.arff('Stamps_withoutdupl_norm_09.arff')
# remove index col
stamps$id=NULL
stamps$outlier = ifelse(stamps$outlier == "yes", 0, 1)
# normalization
# X = apply(subset(stamps, select = -outlier), 2, scale_R)
X = subset(stamps, select = -outlier)
stamps = cbind(X, stamps$outlier)
# remove duplicated rows
stamps = as.matrix(distinct(as.data.frame(stamps)))
# move outliers to the end
stamps = stamps[order(stamps[,10], decreasing=T),]
anyNA(stamps)
##### pageblocks #####
pageblocks = read.arff('PageBlocks_withoutdupl_09.arff')
# remove index col
pageblocks$id=NULL
pageblocks$outlier = ifelse(pageblocks$outlier == "yes", 0, 1)
# normalization
X = apply(subset(pageblocks, select = -outlier), 2, scale_R)
pageblocks = cbind(X, pageblocks$outlier)
# remove duplicated rows
pageblocks = as.matrix(distinct(as.data.frame(pageblocks)))
# move outliers to the end
pageblocks = pageblocks[order(pageblocks[,11], decreasing=T),]
anyNA(pageblocks)
index_outliers = which(pageblocks[,11]==0)
index_regular = which(pageblocks[,11]==1)
set.seed(123)
index_regular_R = sample(index_regular, 4285, replace = FALSE, prob = NULL)
pageblocks = pageblocks[c(index_regular_R, index_outliers),]
remove(index_regular)
remove(index_outliers)
remove(index_regular_R)
##### hepatitis #####
hepatitis = read.arff('Hepatitis_withoutdupl_10_v01.arff')
# remove index col
hepatitis$id=NULL
hepatitis$outlier = ifelse(hepatitis$outlier == "yes", 0, 1)
# normalization
X = apply(subset(hepatitis, select = -outlier), 2, scale)
hepatitis = cbind(X, hepatitis$outlier)
# remove duplicated rows
hepatitis = as.matrix(distinct(as.data.frame(hepatitis)))
# move outliers to the end
hepatitis = hepatitis[order(hepatitis[,20], decreasing=T),]
anyNA(hepatitis)
remove(X)
print("corraltion matrix of ecoli is")
cor(ecoli)
print("corraltion matrix of glass is")
cor(glass)
print("corraltion matrix of hepatitis is")
cor(hepatitis)
print("corraltion matrix of lymphpgrap is")
cor(lymphpgrap)
print("corraltion matrix of ecoli is")
cor(ecoli)
print("corraltion matrix of glass is")
cor(glass)
print("corraltion matrix of hepatitis is")
cor(hepatitis)
print("corraltion matrix of lymphpgraphy is")
cor(lymphpgraphy)
print("corraltion matrix of ecoli is")
cor(ecoli)
print("corraltion matrix of glass is")
cor(glass)
print("corraltion matrix of hepatitis is")
cor(hepatitis)
print("corraltion matrix of lymphography is")
cor(lymphography)
print("corraltion matrix of pageblocks is")
cor(pageblocks)
print("corraltion matrix of PenDigits is")
cor(PenDigits)
print("corraltion matrix of pima is")
cor(pima)
print("corraltion matrix of shuffle is")
cor(shuffle)
print("corraltion matrix of stamps is")
cor(stamps)
print("corraltion matrix of thyroid is")
cor(thyroid)
print("corraltion matrix of vertebral is")
cor(vertebral)
print("corraltion matrix of vowels is")
cor(vowels)
print("corraltion matrix of waveform is")
cor(waveform)
print("corraltion matrix of WBC is")
cor(WBC)
print("corraltion matrix of WDBC is")
cor(WDBC)
print("corraltion matrix of wilt is")
cor(wilt)
# 1 represent regular observations, 0 are outliers
library(R.matlab)
library(foreign)
library(dplyr)
# setwd("/media/rui/exNVME/code_working_folder/Algo_Compare_OD/Real_Datasets")
setwd("G:/code_working_folder/Algo_Compare_OD/Real_Datasets")
# a robust version of normalization
scale_R = function(x){
M = median(x)
madn = mad(x)
return((x-M)/madn)
}
##### WBC #####
WBC = read.arff('WBC_withoutdupl_norm_v02.arff')
View(WBC)
